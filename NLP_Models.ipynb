{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import"
      ],
      "metadata": {
        "id": "ArERjSwHV704"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eawu-AjnVa90",
        "outputId": "7b74ae50-6de5-4b29-c194-e8bd5e54bbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Download the nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the URL to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "\n",
        "# Fetch the content from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Create a corpus list\n",
        "corpus = []\n",
        "\n",
        "# Extract text from each paragraph and clean it\n",
        "for para in paragraphs:\n",
        "    text = para.get_text()\n",
        "    # Simple cleaning to remove references like [1][2]\n",
        "    clean_text = ' '.join(text.replace('\\n', ' ').split())\n",
        "    corpus.append(clean_text)\n",
        "\n",
        "# print corpus\n",
        "for paragraph in corpus:\n",
        "    print(paragraph)\n",
        "    print(\"---------\")\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2qncSYWcpB7",
        "outputId": "b296f5ae-20c4-4e69-cab7-d93a9d6de425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.\n",
            "---------\n",
            "Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n",
            "---------\n",
            "Natural language processing has its roots in the 1940s.[1] Already in 1940, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n",
            "---------\n",
            "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
            "---------\n",
            "Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]\n",
            "---------\n",
            "In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n",
            "---------\n",
            "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\n",
            "---------\n",
            "Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\n",
            "---------\n",
            "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\n",
            "---------\n",
            "Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023.\n",
            "---------\n",
            "Before that they were commonly used:\n",
            "---------\n",
            "In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\n",
            "---------\n",
            "The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n",
            "---------\n",
            "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach was replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.\n",
            "---------\n",
            "Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore.\n",
            "---------\n",
            "Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n",
            "---------\n",
            "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
            "---------\n",
            "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
            "---------\n",
            "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\n",
            "---------\n",
            "Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
            "---------\n",
            "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[47] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n",
            "---------\n",
            "As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:\n",
            "---------\n",
            "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston.\n",
            "---------\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize and normalize case\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Remove stop words and punctuations\n",
        "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply preprocessing to each document in the corpus\n",
        "processed_corpus = [preprocess(text) for text in corpus]\n",
        "\n",
        "# Print the processed corpus\n",
        "for document in processed_corpus:\n",
        "    print(document)\n",
        "    print(\"-----\")\n",
        "print(str(len(processed_corpus)) + ' documents in corpus' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Sjhmm1eiKG",
        "outputId": "590e28b3-fea4-470c-c776-3f31cdb6dc58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'interdisciplinary', 'subfield', 'computer', 'science', 'artificial', 'intelligence', 'primarily', 'concerned', 'providing', 'computer', 'ability', 'process', 'data', 'encoded', 'natural', 'language', 'thus', 'closely', 'related', 'information', 'retrieval', 'knowledge', 'representation', 'computational', 'linguistics', 'subfield', 'linguistics', 'typically', 'data', 'collected', 'text', 'corpus', 'using', 'either', 'statistical', 'approach', 'machine', 'learning', 'deep', 'learning']\n",
            "-----\n",
            "['major', 'task', 'natural', 'language', 'processing', 'speech', 'recognition', 'text', 'classification', 'understanding', 'generation']\n",
            "-----\n",
            "['natural', 'language', 'processing', 'root', '1940s', '1', 'already', '1940', 'alan', 'turing', 'published', 'article', 'titled', 'computing', 'machinery', 'intelligence', 'proposed', 'called', 'turing', 'test', 'criterion', 'intelligence', 'though', 'time', 'articulated', 'problem', 'separate', 'artificial', 'intelligence', 'proposed', 'test', 'includes', 'task', 'involves', 'automated', 'interpretation', 'generation', 'natural', 'language']\n",
            "-----\n",
            "['premise', 'symbolic', 'nlp', 'john', 'searle', 'chinese', 'room', 'experiment', 'given', 'collection', 'rule', 'chinese', 'phrasebook', 'question', 'matching', 'answer', 'computer', 'emulates', 'natural', 'language', 'understanding', 'nlp', 'task', 'applying', 'rule', 'data', 'confronts']\n",
            "-----\n",
            "['1980s', 'natural', 'language', 'processing', 'system', 'based', 'complex', 'set', 'rule', 'starting', 'late', '1980s', 'however', 'revolution', 'natural', 'language', 'processing', 'introduction', 'machine', 'learning', 'algorithm', 'language', 'processing', 'due', 'steady', 'increase', 'computational', 'power', 'see', 'moore', 'law', 'gradual', 'lessening', 'dominance', 'chomskyan', 'theory', 'linguistics', 'transformational', 'grammar', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'sort', 'corpus', 'linguistics', 'underlies', 'approach', 'language', 'processing', '8']\n",
            "-----\n",
            "['2003', 'word', 'model', 'time', 'best', 'statistical', 'algorithm', 'overperformed', 'perceptron', 'single', 'hidden', 'layer', 'context', 'length', 'several', 'word', 'trained', '14', 'million', 'word', 'cpu', 'cluster', 'language', 'modelling', 'yoshua', 'bengio', '9']\n",
            "-----\n",
            "['2010', 'tomáš', 'mikolov', 'phd', 'student', 'brno', 'university', 'technology', 'applied', 'simple', 'recurrent', 'neural', 'network', 'single', 'hidden', 'layer', 'language', 'modelling', '10', 'following', 'year', 'went', 'develop', 'word2vec', '2010s', 'representation', 'learning', 'deep', 'neural', 'featuring', 'many', 'hidden', 'layer', 'machine', 'learning', 'method', 'became', 'widespread', 'natural', 'language', 'processing', 'popularity', 'due', 'partly', 'flurry', 'result', 'showing', 'technique', '11', '12', 'achieve', 'result', 'many', 'natural', 'language', 'task', 'language', 'modeling', '13', 'parsing', '14', '15', 'increasingly', 'important', 'medicine', 'healthcare', 'nlp', 'help', 'analyze', 'note', 'text', 'electronic', 'health', 'record', 'would', 'otherwise', 'inaccessible', 'study', 'seeking', 'improve', 'care', '16', 'protect', 'patient', 'privacy', '17']\n",
            "-----\n",
            "['symbolic', 'approach', 'set', 'rule', 'manipulating', 'symbol', 'coupled', 'dictionary', 'lookup', 'historically', 'first', 'approach', 'used', 'ai', 'general', 'nlp', 'particular', '18', '19', 'writing', 'grammar', 'devising', 'heuristic', 'rule', 'stemming']\n",
            "-----\n",
            "['machine', 'learning', 'approach', 'include', 'statistical', 'neural', 'network', 'hand', 'many', 'advantage', 'symbolic', 'approach']\n",
            "-----\n",
            "['although', 'system', 'manipulating', 'symbol', 'still', 'use', '2020', 'become', 'mostly', 'obsolete', 'advance', 'llm', '2023']\n",
            "-----\n",
            "['commonly', 'used']\n",
            "-----\n",
            "['late', '1980s', 'statistical', 'approach', 'ended', 'period', 'ai', 'winter', 'caused', 'inefficiency', 'approach', '20', '21']\n",
            "-----\n",
            "['earliest', 'decision', 'tree', 'producing', 'system', 'hard', 'rule', 'still', 'similar', 'old', 'approach', 'introduction', 'hidden', 'markov', 'model', 'applied', 'tagging', 'announced', 'end', 'old', 'approach']\n",
            "-----\n",
            "['major', 'drawback', 'statistical', 'method', 'require', 'elaborate', 'feature', 'engineering', 'since', '2015', '22', 'statistical', 'approach', 'replaced', 'neural', 'network', 'approach', 'using', 'semantic', 'network', '23', 'word', 'embeddings', 'capture', 'semantic', 'property', 'word']\n",
            "-----\n",
            "['intermediate', 'task', 'tagging', 'dependency', 'parsing', 'needed', 'anymore']\n",
            "-----\n",
            "['neural', 'machine', 'translation', 'based', 'transformation', 'made', 'obsolete', 'intermediate', 'step', 'word', 'alignment', 'previously', 'necessary', 'statistical', 'machine', 'translation']\n",
            "-----\n",
            "['following', 'list', 'commonly', 'researched', 'task', 'natural', 'language', 'processing', 'task', 'direct', 'application', 'others', 'commonly', 'serve', 'subtasks', 'used', 'aid', 'solving', 'larger', 'task']\n",
            "-----\n",
            "['though', 'natural', 'language', 'processing', 'task', 'closely', 'intertwined', 'subdivided', 'category', 'convenience', 'coarse', 'division', 'given']\n",
            "-----\n",
            "['based', 'trend', 'field', 'possible', 'extrapolate', 'future', 'direction', 'nlp', '2020', 'three', 'trend', 'among', 'topic', 'series', 'conll', 'shared', 'task', 'observed', '46']\n",
            "-----\n",
            "['nlp', 'application', 'involve', 'aspect', 'emulate', 'intelligent', 'behaviour', 'apparent', 'comprehension', 'natural', 'language', 'broadly', 'speaking', 'technical', 'operationalization', 'increasingly', 'advanced', 'aspect', 'cognitive', 'behaviour', 'represents', 'one', 'developmental', 'trajectory', 'nlp', 'see', 'trend', 'among', 'conll', 'shared', 'task']\n",
            "-----\n",
            "['cognition', 'refers', 'mental', 'action', 'process', 'acquiring', 'knowledge', 'understanding', 'thought', 'experience', 'sens', '47', 'cognitive', 'science', 'interdisciplinary', 'scientific', 'study', 'mind', 'process', '48', 'cognitive', 'linguistics', 'interdisciplinary', 'branch', 'linguistics', 'combining', 'knowledge', 'research', 'psychology', 'linguistics', '49', 'especially', 'age', 'symbolic', 'nlp', 'area', 'computational', 'linguistics', 'maintained', 'strong', 'tie', 'cognitive', 'study']\n",
            "-----\n",
            "['example', 'george', 'lakoff', 'offer', 'methodology', 'build', 'natural', 'language', 'processing', 'nlp', 'algorithm', 'perspective', 'cognitive', 'science', 'along', 'finding', 'cognitive', 'linguistics', '50', 'two', 'defining', 'aspect']\n",
            "-----\n",
            "['tie', 'cognitive', 'linguistics', 'part', 'historical', 'heritage', 'nlp', 'le', 'frequently', 'addressed', 'since', 'statistical', 'turn', '1990s', 'nevertheless', 'approach', 'develop', 'cognitive', 'model', 'towards', 'technically', 'operationalizable', 'framework', 'pursued', 'context', 'various', 'framework', 'cognitive', 'grammar', '53', 'functional', 'grammar', '54', 'construction', 'grammar', '55', 'computational', 'psycholinguistics', 'cognitive', 'neuroscience', 'however', 'limited', 'uptake', 'mainstream', 'nlp', 'measured', 'presence', 'major', 'conference', '56', 'acl', 'recently', 'idea', 'cognitive', 'nlp', 'revived', 'approach', 'achieve', 'explainability', 'notion', 'cognitive', 'ai', '57', 'likewise', 'idea', 'cognitive', 'nlp', 'inherent', 'neural', 'model', 'multimodal', 'nlp', 'although', 'rarely', 'made', 'explicit', '58', 'development', 'artificial', 'intelligence', 'specifically', 'tool', 'technology', 'using', 'large', 'language', 'model', 'approach', '59', 'new', 'direction', 'artificial', 'general', 'intelligence', 'based', 'free', 'energy', 'principle', '60', 'british', 'neuroscientist', 'theoretician', 'university', 'college', 'london', 'karl', 'friston']\n",
            "-----\n",
            "23 documents in corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train a Word2Vec model\n",
        "embedded_words = Word2Vec(sentences=processed_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
        "embedded_words.save(\"word2vec.model\")\n",
        "# Use the model to get the vector of a word\n",
        "word_vector = embedded_words.wv['nlp']  # 'nlp' is the word for which you want the vector\n",
        "\n",
        "# Output the vector\n",
        "print(word_vector)\n",
        "\n",
        "print(embedded_words.wv.most_similar('nlp'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kod5qWqeiA8",
        "outputId": "66491b80-6cc4-4c9d-afc8-603f51313452"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01711663  0.00727925  0.01039552  0.01158247  0.01476423 -0.01260179\n",
            "  0.00225464  0.01256732 -0.0061258  -0.0124349  -0.00091152 -0.01682046\n",
            " -0.0110077   0.01431822  0.00634701  0.01457427  0.01393051  0.01519224\n",
            " -0.00788206 -0.00125845  0.00451408 -0.00897556  0.01715351 -0.01984543\n",
            "  0.01385504  0.00608646 -0.00993883  0.00884507 -0.00406971  0.01363702\n",
            "  0.02010466 -0.00862695 -0.00138743 -0.01128995  0.00740453  0.00602722\n",
            "  0.01401703  0.01228605  0.01909166  0.01818954  0.01611374 -0.0140514\n",
            " -0.01852454 -0.00046264 -0.00545288  0.01591346  0.01197275 -0.00340572\n",
            "  0.00306722  0.00361921]\n",
            "[('speech', 0.3672628104686737), ('best', 0.3532448410987854), ('intermediate', 0.3450145125389099), ('moore', 0.3342324197292328), ('given', 0.3302149772644043), ('healthcare', 0.32867226004600525), ('subdivided', 0.3087236285209656), ('21', 0.3025331497192383), ('experiment', 0.30020180344581604), ('replaced', 0.29874563217163086)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEN0rWkVeh1Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "\n",
        "Predicting the next word in a text with the straight forward aproach of assigning index to each word."
      ],
      "metadata": {
        "id": "ClkVKbosj9zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# 'processed_corpus' is available from previous steps\n",
        "\n",
        "# Tokenization and sequence creation\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(processed_corpus)\n",
        "sequences = tokenizer.texts_to_sequences(processed_corpus)\n",
        "print(sequences)\n",
        "\n",
        "\n",
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "print(input_sequences)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "print(len(input_sequences))\n",
        "print(len(input_sequences[0])) # all sequences in the same lenght\n",
        "\n",
        "# Prepare predictors and labels\n",
        "predictors, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "labels = to_categorical(labels, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "print(predictors[0])\n",
        "print(labels[0])\n",
        "\n",
        "\n",
        "# Building the RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=30, input_length=max_sequence_len - 1),\n",
        "    SimpleRNN(150, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    SimpleRNN(100),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Training the model\n",
        "#model.fit(predictors, labels, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3K6LpwZfzjM",
        "outputId": "54383ffa-a998-42f3-db42-6a70f22d15cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3, 11, 12, 49, 12], [32, 7, 4, 1, 6, 113, 114, 30, 115, 33, 50], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134, 135, 50, 4, 1], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7, 147, 13, 28, 148], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168, 3, 1, 6, 169], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1, 67, 180, 181, 182], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224, 225, 226, 227, 228], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16, 238, 239, 13, 240], [11, 12, 3, 241, 9, 15, 23, 242, 38, 243, 20, 3], [81, 35, 78, 79, 82, 244, 83, 245, 246, 84, 247, 248, 249], [42, 40], [59, 34, 9, 3, 250, 251, 41, 252, 253, 254, 3, 255, 256], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86, 264, 265, 85, 3], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275, 276, 88, 277, 14], [89, 7, 86, 278, 76, 279, 280], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284, 285, 9, 11, 90], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40, 292, 293, 294, 7], [54, 4, 1, 6, 7, 46, 295, 296, 297, 298, 299, 300, 57], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95, 96, 7, 308, 309], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43, 94, 95, 96, 7], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344, 345, 98, 5, 39], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8, 355, 356, 357, 44], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68, 411, 412, 413, 414]]\n",
            "[[4, 1], [4, 1, 6], [4, 1, 6, 2], [4, 1, 6, 2, 24], [4, 1, 6, 2, 24, 45], [4, 1, 6, 2, 24, 45, 25], [4, 1, 6, 2, 24, 45, 25, 26], [4, 1, 6, 2, 24, 45, 25, 26, 18], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3, 11], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3, 11, 12], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3, 11, 12, 49], [4, 1, 6, 2, 24, 45, 25, 26, 18, 10, 101, 102, 103, 25, 104, 27, 28, 105, 4, 1, 106, 46, 107, 108, 109, 29, 47, 19, 8, 45, 8, 110, 28, 111, 30, 48, 31, 112, 9, 3, 11, 12, 49, 12], [32, 7], [32, 7, 4], [32, 7, 4, 1], [32, 7, 4, 1, 6], [32, 7, 4, 1, 6, 113], [32, 7, 4, 1, 6, 113, 114], [32, 7, 4, 1, 6, 113, 114, 30], [32, 7, 4, 1, 6, 113, 114, 30, 115], [32, 7, 4, 1, 6, 113, 114, 30, 115, 33], [32, 7, 4, 1, 6, 113, 114, 30, 115, 33, 50], [4, 1], [4, 1, 6], [4, 1, 6, 116], [4, 1, 6, 116, 117], [4, 1, 6, 116, 117, 118], [4, 1, 6, 116, 117, 118, 119], [4, 1, 6, 116, 117, 118, 119, 120], [4, 1, 6, 116, 117, 118, 119, 120, 121], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134, 135], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134, 135, 50], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134, 135, 50, 4], [4, 1, 6, 116, 117, 118, 119, 120, 121, 51, 122, 123, 124, 125, 126, 10, 52, 127, 51, 53, 128, 10, 54, 55, 129, 130, 131, 18, 10, 52, 53, 132, 7, 133, 134, 135, 50, 4, 1], [136, 20], [136, 20, 2], [136, 20, 2, 137], [136, 20, 2, 137, 138], [136, 20, 2, 137, 138, 56], [136, 20, 2, 137, 138, 56, 139], [136, 20, 2, 137, 138, 56, 139, 140], [136, 20, 2, 137, 138, 56, 139, 140, 57], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7, 147], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7, 147, 13], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7, 147, 13, 28], [136, 20, 2, 137, 138, 56, 139, 140, 57, 141, 13, 56, 142, 143, 144, 145, 25, 146, 4, 1, 33, 2, 7, 147, 13, 28, 148], [34, 4], [34, 4, 1], [34, 4, 1, 6], [34, 4, 1, 6, 35], [34, 4, 1, 6, 35, 21], [34, 4, 1, 6, 35, 21, 149], [34, 4, 1, 6, 35, 21, 149, 58], [34, 4, 1, 6, 35, 21, 149, 58, 13], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168, 3], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168, 3, 1], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168, 3, 1, 6], [34, 4, 1, 6, 35, 21, 149, 58, 13, 150, 59, 34, 60, 151, 4, 1, 6, 61, 11, 12, 36, 1, 6, 62, 152, 153, 19, 154, 63, 155, 156, 157, 158, 159, 160, 161, 8, 162, 16, 163, 164, 165, 166, 167, 48, 8, 168, 3, 1, 6, 169], [170, 14], [170, 14, 17], [170, 14, 17, 55], [170, 14, 17, 55, 171], [170, 14, 17, 55, 171, 9], [170, 14, 17, 55, 171, 9, 36], [170, 14, 17, 55, 171, 9, 36, 172], [170, 14, 17, 55, 171, 9, 36, 172, 173], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1, 67], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1, 67, 180], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1, 67, 180, 181], [170, 14, 17, 55, 171, 9, 36, 172, 173, 64, 22, 37, 65, 174, 175, 14, 176, 66, 177, 14, 178, 179, 1, 67, 180, 181, 182], [183, 184], [183, 184, 185], [183, 184, 185, 186], [183, 184, 185, 186, 187], [183, 184, 185, 186, 187, 188], [183, 184, 185, 186, 187, 188, 68], [183, 184, 185, 186, 187, 188, 68, 69], [183, 184, 185, 186, 187, 188, 68, 69, 70], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224, 225], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224, 225, 226], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224, 225, 226, 227], [183, 184, 185, 186, 187, 188, 68, 69, 70, 189, 190, 15, 23, 64, 22, 37, 1, 67, 191, 71, 192, 193, 72, 194, 195, 47, 12, 49, 15, 196, 38, 22, 37, 11, 12, 73, 197, 198, 4, 1, 6, 199, 62, 200, 201, 74, 202, 203, 204, 205, 75, 74, 38, 4, 1, 7, 1, 206, 207, 76, 66, 208, 77, 209, 210, 211, 2, 212, 213, 214, 30, 215, 216, 217, 218, 219, 220, 39, 221, 222, 223, 224, 225, 226, 227, 228], [20, 3], [20, 3, 58], [20, 3, 58, 13], [20, 3, 58, 13, 78], [20, 3, 58, 13, 78, 79], [20, 3, 58, 13, 78, 79, 229], [20, 3, 58, 13, 78, 79, 229, 230], [20, 3, 58, 13, 78, 79, 229, 230, 231], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16, 238], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16, 238, 239], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16, 238, 239, 13], [20, 3, 58, 13, 78, 79, 229, 230, 231, 232, 233, 3, 40, 41, 80, 2, 234, 235, 236, 237, 16, 238, 239, 13, 240], [11, 12], [11, 12, 3], [11, 12, 3, 241], [11, 12, 3, 241, 9], [11, 12, 3, 241, 9, 15], [11, 12, 3, 241, 9, 15, 23], [11, 12, 3, 241, 9, 15, 23, 242], [11, 12, 3, 241, 9, 15, 23, 242, 38], [11, 12, 3, 241, 9, 15, 23, 242, 38, 243], [11, 12, 3, 241, 9, 15, 23, 242, 38, 243, 20], [11, 12, 3, 241, 9, 15, 23, 242, 38, 243, 20, 3], [81, 35], [81, 35, 78], [81, 35, 78, 79], [81, 35, 78, 79, 82], [81, 35, 78, 79, 82, 244], [81, 35, 78, 79, 82, 244, 83], [81, 35, 78, 79, 82, 244, 83, 245], [81, 35, 78, 79, 82, 244, 83, 245, 246], [81, 35, 78, 79, 82, 244, 83, 245, 246, 84], [81, 35, 78, 79, 82, 244, 83, 245, 246, 84, 247], [81, 35, 78, 79, 82, 244, 83, 245, 246, 84, 247, 248], [81, 35, 78, 79, 82, 244, 83, 245, 246, 84, 247, 248, 249], [42, 40], [59, 34], [59, 34, 9], [59, 34, 9, 3], [59, 34, 9, 3, 250], [59, 34, 9, 3, 250, 251], [59, 34, 9, 3, 250, 251, 41], [59, 34, 9, 3, 250, 251, 41, 252], [59, 34, 9, 3, 250, 251, 41, 252, 253], [59, 34, 9, 3, 250, 251, 41, 252, 253, 254], [59, 34, 9, 3, 250, 251, 41, 252, 253, 254, 3], [59, 34, 9, 3, 250, 251, 41, 252, 253, 254, 3, 255], [59, 34, 9, 3, 250, 251, 41, 252, 253, 254, 3, 255, 256], [257, 258], [257, 258, 259], [257, 258, 259, 260], [257, 258, 259, 260, 35], [257, 258, 259, 260, 35, 261], [257, 258, 259, 260, 35, 261, 13], [257, 258, 259, 260, 35, 261, 13, 82], [257, 258, 259, 260, 35, 261, 13, 82, 262], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86, 264], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86, 264, 265], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86, 264, 265, 85], [257, 258, 259, 260, 35, 261, 13, 82, 262, 85, 3, 61, 22, 263, 17, 70, 86, 264, 265, 85, 3], [32, 266], [32, 266, 9], [32, 266, 9, 73], [32, 266, 9, 73, 267], [32, 266, 9, 73, 267, 268], [32, 266, 9, 73, 267, 268, 269], [32, 266, 9, 73, 267, 268, 269, 270], [32, 266, 9, 73, 267, 268, 269, 270, 87], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275, 276], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275, 276, 88], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275, 276, 88, 277], [32, 266, 9, 73, 267, 268, 269, 270, 87, 271, 272, 9, 3, 273, 15, 23, 3, 31, 88, 23, 274, 14, 275, 276, 88, 277, 14], [89, 7], [89, 7, 86], [89, 7, 86, 278], [89, 7, 86, 278, 76], [89, 7, 86, 278, 76, 279], [89, 7, 86, 278, 76, 279, 280], [15, 11], [15, 11, 90], [15, 11, 90, 21], [15, 11, 90, 21, 281], [15, 11, 90, 21, 281, 91], [15, 11, 90, 21, 281, 91, 84], [15, 11, 90, 21, 281, 91, 84, 89], [15, 11, 90, 21, 281, 91, 84, 89, 282], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284, 285], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284, 285, 9], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284, 285, 9, 11], [15, 11, 90, 21, 281, 91, 84, 89, 282, 14, 283, 284, 285, 9, 11, 90], [71, 286], [71, 286, 42], [71, 286, 42, 287], [71, 286, 42, 287, 7], [71, 286, 42, 287, 7, 4], [71, 286, 42, 287, 7, 4, 1], [71, 286, 42, 287, 7, 4, 1, 6], [71, 286, 42, 287, 7, 4, 1, 6, 7], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40, 292], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40, 292, 293], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40, 292, 293, 294], [71, 286, 42, 287, 7, 4, 1, 6, 7, 288, 92, 289, 42, 290, 291, 40, 292, 293, 294, 7], [54, 4], [54, 4, 1], [54, 4, 1, 6], [54, 4, 1, 6, 7], [54, 4, 1, 6, 7, 46], [54, 4, 1, 6, 7, 46, 295], [54, 4, 1, 6, 7, 46, 295, 296], [54, 4, 1, 6, 7, 46, 295, 296, 297], [54, 4, 1, 6, 7, 46, 295, 296, 297, 298], [54, 4, 1, 6, 7, 46, 295, 296, 297, 298, 299], [54, 4, 1, 6, 7, 46, 295, 296, 297, 298, 299, 300], [54, 4, 1, 6, 7, 46, 295, 296, 297, 298, 299, 300, 57], [21, 43], [21, 43, 301], [21, 43, 301, 302], [21, 43, 301, 302, 303], [21, 43, 301, 302, 303, 304], [21, 43, 301, 302, 303, 304, 93], [21, 43, 301, 302, 303, 304, 93, 2], [21, 43, 301, 302, 303, 304, 93, 2, 83], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95, 96], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95, 96, 7], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95, 96, 7, 308], [21, 43, 301, 302, 303, 304, 93, 2, 83, 305, 43, 94, 306, 307, 95, 96, 7, 308, 309], [2, 92], [2, 92, 310], [2, 92, 310, 44], [2, 92, 310, 44, 311], [2, 92, 310, 44, 311, 312], [2, 92, 310, 44, 311, 312, 97], [2, 92, 310, 44, 311, 312, 97, 313], [2, 92, 310, 44, 311, 312, 97, 313, 314], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43, 94], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43, 94, 95], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43, 94, 95, 96], [2, 92, 310, 44, 311, 312, 97, 313, 314, 4, 1, 315, 316, 317, 318, 77, 319, 44, 5, 97, 320, 321, 322, 323, 2, 63, 43, 94, 95, 96, 7], [324, 325], [324, 325, 326], [324, 325, 326, 327], [324, 325, 326, 327, 27], [324, 325, 326, 327, 27, 328], [324, 325, 326, 327, 27, 328, 29], [324, 325, 326, 327, 27, 328, 29, 33], [324, 325, 326, 327, 27, 328, 29, 33, 329], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344, 345], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344, 345, 98], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344, 345, 98, 5], [324, 325, 326, 327, 27, 328, 29, 33, 329, 330, 331, 332, 5, 26, 24, 333, 39, 334, 27, 335, 5, 8, 24, 336, 8, 337, 29, 338, 339, 8, 340, 341, 342, 20, 2, 343, 19, 8, 344, 345, 98, 5, 39], [346, 347], [346, 347, 348], [346, 347, 348, 349], [346, 347, 348, 349, 350], [346, 347, 348, 349, 350, 351], [346, 347, 348, 349, 350, 351, 4], [346, 347, 348, 349, 350, 351, 4, 1], [346, 347, 348, 349, 350, 351, 4, 1, 6], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8, 355], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8, 355, 356], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8, 355, 356, 357], [346, 347, 348, 349, 350, 351, 4, 1, 6, 2, 36, 352, 5, 26, 353, 354, 5, 8, 355, 356, 357, 44], [98, 5], [98, 5, 8], [98, 5, 8, 358], [98, 5, 8, 358, 359], [98, 5, 8, 358, 359, 360], [98, 5, 8, 358, 359, 360, 2], [98, 5, 8, 358, 359, 360, 2, 361], [98, 5, 8, 358, 359, 360, 2, 361, 362], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68, 411], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68, 411, 412], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68, 411, 412, 413], [98, 5, 8, 358, 359, 360, 2, 361, 362, 363, 87, 9, 364, 365, 366, 3, 72, 5, 17, 367, 368, 369, 99, 370, 65, 371, 99, 5, 16, 372, 373, 16, 374, 375, 16, 376, 19, 377, 5, 378, 60, 379, 380, 381, 2, 382, 383, 32, 384, 385, 386, 387, 100, 5, 2, 388, 3, 75, 389, 390, 5, 41, 391, 392, 100, 5, 2, 393, 15, 17, 394, 2, 81, 395, 91, 396, 397, 398, 18, 10, 399, 400, 69, 31, 401, 1, 17, 3, 402, 403, 93, 18, 80, 10, 21, 404, 405, 406, 407, 408, 409, 410, 68, 411, 412, 413, 414]]\n",
            "653\n",
            "107\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 106, 30)           12450     \n",
            "                                                                 \n",
            " simple_rnn_10 (SimpleRNN)   (None, 106, 150)          27150     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 106, 150)          0         \n",
            "                                                                 \n",
            " simple_rnn_11 (SimpleRNN)   (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 415)               41915     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 106615 (416.46 KB)\n",
            "Trainable params: 106615 (416.46 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, tokenizer, seed_text, max_sequence_len):\n",
        "    # Tokenize and pad the seed text to fit the model's input format\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "    # Predict the class probabilities for the next word\n",
        "    probabilities = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "    # Get the index of the most probable next word\n",
        "    predicted_index = np.argmax(probabilities)\n",
        "\n",
        "    # Map the index to the corresponding word\n",
        "    predicted_word = None\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            predicted_word = word\n",
        "            break\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "seed_text = \"Deep learning models\"\n",
        "for i in range(10):\n",
        "\n",
        "    next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len)\n",
        "    seed_text += \" \" + next_word\n",
        "    print(\"text: \", seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVnM_AEYfzgf",
        "outputId": "656dc193-41db-49d3-a911-23fd42ed1078"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  Deep learning models approach\n",
            "text:  Deep learning models approach include\n",
            "text:  Deep learning models approach include statistical\n",
            "text:  Deep learning models approach include statistical neural\n",
            "text:  Deep learning models approach include statistical neural network\n",
            "text:  Deep learning models approach include statistical neural network hand\n",
            "text:  Deep learning models approach include statistical neural network hand many\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage symbolic\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage symbolic approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN - word2vec"
      ],
      "metadata": {
        "id": "Mtuv9woOepMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming embedded_words is your Word2Vec model\n",
        "word_vectors = embedded_words.wv  # Access the word vectors directly\n",
        "\n",
        "# Prepare the embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word_vectors.vector_size), dtype='float32')\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    # Check if the word is in the word_vectors\n",
        "    if word in word_vectors.key_to_index:\n",
        "        # If word is in the model, retrieve the corresponding vector\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "\n",
        "# Building the RNN Model with Word2Vec embeddings\n",
        "model_word2vec_RNN = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=word_vectors.vector_size, weights=[embedding_matrix], input_length=max_sequence_len - 1, trainable=True),\n",
        "    SimpleRNN(150, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    SimpleRNN(100),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "model_word2vec_RNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_word2vec_RNN.summary())\n",
        "# Training the model\n",
        "model_word2vec_RNN.fit(predictors, labels, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "id": "qcWS7jfKetXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2c0f5a-0fb1-43f7-ccab-aaa0fa01aba8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 106, 50)           20750     \n",
            "                                                                 \n",
            " simple_rnn_12 (SimpleRNN)   (None, 106, 150)          30150     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 106, 150)          0         \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 415)               41915     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 117915 (460.61 KB)\n",
            "Trainable params: 117915 (460.61 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "21/21 [==============================] - 4s 79ms/step - loss: 6.0478 - accuracy: 0.0107\n",
            "Epoch 2/100\n",
            "21/21 [==============================] - 2s 78ms/step - loss: 5.9506 - accuracy: 0.0107\n",
            "Epoch 3/100\n",
            "21/21 [==============================] - 2s 119ms/step - loss: 5.8587 - accuracy: 0.0184\n",
            "Epoch 4/100\n",
            "21/21 [==============================] - 2s 114ms/step - loss: 5.7873 - accuracy: 0.0245\n",
            "Epoch 5/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 5.7209 - accuracy: 0.0245\n",
            "Epoch 6/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 5.6594 - accuracy: 0.0383\n",
            "Epoch 7/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 5.6230 - accuracy: 0.0337\n",
            "Epoch 8/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 5.4529 - accuracy: 0.0689\n",
            "Epoch 9/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 5.2459 - accuracy: 0.0888\n",
            "Epoch 10/100\n",
            "21/21 [==============================] - 2s 98ms/step - loss: 5.0259 - accuracy: 0.1424\n",
            "Epoch 11/100\n",
            "21/21 [==============================] - 3s 146ms/step - loss: 4.7774 - accuracy: 0.2098\n",
            "Epoch 12/100\n",
            "21/21 [==============================] - 2s 106ms/step - loss: 4.4978 - accuracy: 0.2910\n",
            "Epoch 13/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 4.2258 - accuracy: 0.3522\n",
            "Epoch 14/100\n",
            "21/21 [==============================] - 3s 135ms/step - loss: 3.9563 - accuracy: 0.4472\n",
            "Epoch 15/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 3.6757 - accuracy: 0.4962\n",
            "Epoch 16/100\n",
            "21/21 [==============================] - 2s 85ms/step - loss: 3.4086 - accuracy: 0.5681\n",
            "Epoch 17/100\n",
            "21/21 [==============================] - 3s 134ms/step - loss: 3.1321 - accuracy: 0.6233\n",
            "Epoch 18/100\n",
            "21/21 [==============================] - 2s 108ms/step - loss: 2.8687 - accuracy: 0.6937\n",
            "Epoch 19/100\n",
            "21/21 [==============================] - 2s 103ms/step - loss: 2.6168 - accuracy: 0.7320\n",
            "Epoch 20/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 2.3697 - accuracy: 0.7596\n",
            "Epoch 21/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 2.1509 - accuracy: 0.7994\n",
            "Epoch 22/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 1.9481 - accuracy: 0.8254\n",
            "Epoch 23/100\n",
            "21/21 [==============================] - 2s 87ms/step - loss: 1.7638 - accuracy: 0.8622\n",
            "Epoch 24/100\n",
            "21/21 [==============================] - 3s 132ms/step - loss: 1.5723 - accuracy: 0.8882\n",
            "Epoch 25/100\n",
            "21/21 [==============================] - 2s 94ms/step - loss: 1.4080 - accuracy: 0.8913\n",
            "Epoch 26/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 1.2692 - accuracy: 0.9112\n",
            "Epoch 27/100\n",
            "21/21 [==============================] - 2s 77ms/step - loss: 1.1505 - accuracy: 0.9250\n",
            "Epoch 28/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 1.0583 - accuracy: 0.9265\n",
            "Epoch 29/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.9399 - accuracy: 0.9342\n",
            "Epoch 30/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.8500 - accuracy: 0.9449\n",
            "Epoch 31/100\n",
            "21/21 [==============================] - 2s 120ms/step - loss: 0.7725 - accuracy: 0.9464\n",
            "Epoch 32/100\n",
            "21/21 [==============================] - 2s 115ms/step - loss: 0.6935 - accuracy: 0.9479\n",
            "Epoch 33/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.6303 - accuracy: 0.9602\n",
            "Epoch 34/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.5731 - accuracy: 0.9587\n",
            "Epoch 35/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.5305 - accuracy: 0.9663\n",
            "Epoch 36/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.4903 - accuracy: 0.9694\n",
            "Epoch 37/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.4692 - accuracy: 0.9632\n",
            "Epoch 38/100\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.4357 - accuracy: 0.9663\n",
            "Epoch 39/100\n",
            "21/21 [==============================] - 3s 129ms/step - loss: 0.4037 - accuracy: 0.9724\n",
            "Epoch 40/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.3669 - accuracy: 0.9694\n",
            "Epoch 41/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.3421 - accuracy: 0.9740\n",
            "Epoch 42/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.3159 - accuracy: 0.9786\n",
            "Epoch 43/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.2908 - accuracy: 0.9770\n",
            "Epoch 44/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.2748 - accuracy: 0.9786\n",
            "Epoch 45/100\n",
            "21/21 [==============================] - 2s 87ms/step - loss: 0.2691 - accuracy: 0.9770\n",
            "Epoch 46/100\n",
            "21/21 [==============================] - 3s 135ms/step - loss: 0.2474 - accuracy: 0.9801\n",
            "Epoch 47/100\n",
            "21/21 [==============================] - 2s 88ms/step - loss: 0.2312 - accuracy: 0.9832\n",
            "Epoch 48/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.2140 - accuracy: 0.9832\n",
            "Epoch 49/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.2019 - accuracy: 0.9893\n",
            "Epoch 50/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.1946 - accuracy: 0.9862\n",
            "Epoch 51/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.1854 - accuracy: 0.9877\n",
            "Epoch 52/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.1749 - accuracy: 0.9877\n",
            "Epoch 53/100\n",
            "21/21 [==============================] - 3s 131ms/step - loss: 0.1638 - accuracy: 0.9893\n",
            "Epoch 54/100\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.1584 - accuracy: 0.9893\n",
            "Epoch 55/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.1521 - accuracy: 0.9893\n",
            "Epoch 56/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.1407 - accuracy: 0.9923\n",
            "Epoch 57/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.1329 - accuracy: 0.9954\n",
            "Epoch 58/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.1310 - accuracy: 0.9939\n",
            "Epoch 59/100\n",
            "21/21 [==============================] - 2s 78ms/step - loss: 0.1228 - accuracy: 0.9939\n",
            "Epoch 60/100\n",
            "21/21 [==============================] - 2s 107ms/step - loss: 0.1126 - accuracy: 0.9969\n",
            "Epoch 61/100\n",
            "21/21 [==============================] - 3s 120ms/step - loss: 0.1098 - accuracy: 0.9969\n",
            "Epoch 62/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 0.1047 - accuracy: 0.9954\n",
            "Epoch 63/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 0.1071 - accuracy: 0.9954\n",
            "Epoch 64/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.0998 - accuracy: 0.9954\n",
            "Epoch 65/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0927 - accuracy: 0.9954\n",
            "Epoch 66/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0895 - accuracy: 0.9939\n",
            "Epoch 67/100\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0859 - accuracy: 0.9954\n",
            "Epoch 68/100\n",
            "21/21 [==============================] - 3s 131ms/step - loss: 0.0830 - accuracy: 0.9969\n",
            "Epoch 69/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0795 - accuracy: 0.9954\n",
            "Epoch 70/100\n",
            "21/21 [==============================] - 2s 83ms/step - loss: 0.0734 - accuracy: 0.9969\n",
            "Epoch 71/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.0705 - accuracy: 0.9954\n",
            "Epoch 72/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.0673 - accuracy: 0.9985\n",
            "Epoch 73/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 0.0662 - accuracy: 0.9969\n",
            "Epoch 74/100\n",
            "21/21 [==============================] - 2s 85ms/step - loss: 0.0634 - accuracy: 0.9954\n",
            "Epoch 75/100\n",
            "21/21 [==============================] - 3s 132ms/step - loss: 0.0621 - accuracy: 0.9969\n",
            "Epoch 76/100\n",
            "21/21 [==============================] - 2s 96ms/step - loss: 0.0652 - accuracy: 0.9969\n",
            "Epoch 77/100\n",
            "21/21 [==============================] - 3s 132ms/step - loss: 0.0637 - accuracy: 0.9954\n",
            "Epoch 78/100\n",
            "21/21 [==============================] - 2s 92ms/step - loss: 0.0582 - accuracy: 0.9969\n",
            "Epoch 79/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 0.0569 - accuracy: 0.9954\n",
            "Epoch 80/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0556 - accuracy: 0.9954\n",
            "Epoch 81/100\n",
            "21/21 [==============================] - 2s 113ms/step - loss: 0.0511 - accuracy: 0.9969\n",
            "Epoch 82/100\n",
            "21/21 [==============================] - 3s 125ms/step - loss: 0.0518 - accuracy: 0.9939\n",
            "Epoch 83/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.0485 - accuracy: 0.9969\n",
            "Epoch 84/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0458 - accuracy: 0.9954\n",
            "Epoch 85/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0449 - accuracy: 0.9969\n",
            "Epoch 86/100\n",
            "21/21 [==============================] - 2s 80ms/step - loss: 0.0431 - accuracy: 0.9954\n",
            "Epoch 87/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.0418 - accuracy: 0.9954\n",
            "Epoch 88/100\n",
            "21/21 [==============================] - 2s 99ms/step - loss: 0.0398 - accuracy: 0.9969\n",
            "Epoch 89/100\n",
            "21/21 [==============================] - 3s 131ms/step - loss: 0.0405 - accuracy: 0.9954\n",
            "Epoch 90/100\n",
            "21/21 [==============================] - 2s 83ms/step - loss: 0.0385 - accuracy: 0.9954\n",
            "Epoch 91/100\n",
            "21/21 [==============================] - 2s 81ms/step - loss: 0.0377 - accuracy: 0.9954\n",
            "Epoch 92/100\n",
            "21/21 [==============================] - 2s 78ms/step - loss: 0.0385 - accuracy: 0.9954\n",
            "Epoch 93/100\n",
            "21/21 [==============================] - 2s 79ms/step - loss: 0.0386 - accuracy: 0.9954\n",
            "Epoch 94/100\n",
            "21/21 [==============================] - 2s 82ms/step - loss: 0.0343 - accuracy: 0.9954\n",
            "Epoch 95/100\n",
            "21/21 [==============================] - 2s 78ms/step - loss: 0.0345 - accuracy: 0.9954\n",
            "Epoch 96/100\n",
            "21/21 [==============================] - 3s 129ms/step - loss: 0.0330 - accuracy: 0.9954\n",
            "Epoch 97/100\n",
            "21/21 [==============================] - 2s 100ms/step - loss: 0.0323 - accuracy: 0.9954\n",
            "Epoch 98/100\n",
            "21/21 [==============================] - 2s 77ms/step - loss: 0.0310 - accuracy: 0.9969\n",
            "Epoch 99/100\n",
            "21/21 [==============================] - 2s 78ms/step - loss: 0.0301 - accuracy: 0.9969\n",
            "Epoch 100/100\n",
            "21/21 [==============================] - 2s 83ms/step - loss: 0.0313 - accuracy: 0.9969\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7df0c9c5b310>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Deep learning models\"\n",
        "for i in range(10):\n",
        "\n",
        "    next_word = predict_next_word(model_word2vec_RNN, tokenizer, seed_text, max_sequence_len)\n",
        "    seed_text += \" \" + next_word\n",
        "    print(\"text: \", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THoPb_VKtWFk",
        "outputId": "a7a90d2d-ebd4-47b3-90b8-93dbf189f7a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 29 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7df0c9cef130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  Deep learning models approach\n",
            "text:  Deep learning models approach include\n",
            "text:  Deep learning models approach include statistical\n",
            "text:  Deep learning models approach include statistical neural\n",
            "text:  Deep learning models approach include statistical neural network\n",
            "text:  Deep learning models approach include statistical neural network hand\n",
            "text:  Deep learning models approach include statistical neural network hand many\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage symbolic\n",
            "text:  Deep learning models approach include statistical neural network hand many advantage symbolic approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now i will try to train with LSTM and word2vec embedding to see differences."
      ],
      "metadata": {
        "id": "yyPXPCnPteUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming embedded_words is your Word2Vec model\n",
        "word_vectors = embedded_words.wv  # Access the word vectors directly\n",
        "\n",
        "# Prepare the embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word_vectors.vector_size), dtype='float32')\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    # Check if the word is in the word_vectors\n",
        "    if word in word_vectors.key_to_index:\n",
        "        # If word is in the model, retrieve the corresponding vector\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "\n",
        "# Building the RNN Model with Word2Vec embeddings\n",
        "model_word2vec_LSTM = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=word_vectors.vector_size, weights=[embedding_matrix], input_length=max_sequence_len - 1, trainable=True),\n",
        "    LSTM(150, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(100),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "model_word2vec_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_word2vec_LSTM.summary())\n",
        "# Training the model\n",
        "model_word2vec_LSTM.fit(predictors, labels, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-AsxG4ftpzS",
        "outputId": "a2527bad-1334-41c4-8dac-688e1cf2798b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 106, 50)           20750     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 106, 150)          120600    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 106, 150)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               100400    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 415)               41915     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 283665 (1.08 MB)\n",
            "Trainable params: 283665 (1.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "21/21 [==============================] - 9s 234ms/step - loss: 6.0134 - accuracy: 0.0245\n",
            "Epoch 2/100\n",
            "21/21 [==============================] - 7s 326ms/step - loss: 5.9110 - accuracy: 0.0306\n",
            "Epoch 3/100\n",
            "21/21 [==============================] - 5s 233ms/step - loss: 5.7656 - accuracy: 0.0306\n",
            "Epoch 4/100\n",
            "21/21 [==============================] - 5s 241ms/step - loss: 5.7340 - accuracy: 0.0306\n",
            "Epoch 5/100\n",
            "21/21 [==============================] - 7s 308ms/step - loss: 5.7194 - accuracy: 0.0306\n",
            "Epoch 6/100\n",
            "21/21 [==============================] - 5s 234ms/step - loss: 5.7143 - accuracy: 0.0306\n",
            "Epoch 7/100\n",
            "21/21 [==============================] - 7s 325ms/step - loss: 5.7109 - accuracy: 0.0306\n",
            "Epoch 8/100\n",
            "21/21 [==============================] - 5s 234ms/step - loss: 5.7055 - accuracy: 0.0306\n",
            "Epoch 9/100\n",
            "21/21 [==============================] - 6s 284ms/step - loss: 5.7029 - accuracy: 0.0306\n",
            "Epoch 10/100\n",
            "21/21 [==============================] - 6s 267ms/step - loss: 5.6923 - accuracy: 0.0306\n",
            "Epoch 11/100\n",
            "21/21 [==============================] - 5s 235ms/step - loss: 5.6756 - accuracy: 0.0306\n",
            "Epoch 12/100\n",
            "21/21 [==============================] - 11s 529ms/step - loss: 5.6196 - accuracy: 0.0306\n",
            "Epoch 13/100\n",
            "21/21 [==============================] - 9s 411ms/step - loss: 5.5277 - accuracy: 0.0306\n",
            "Epoch 14/100\n",
            "21/21 [==============================] - 5s 237ms/step - loss: 5.4297 - accuracy: 0.0306\n",
            "Epoch 15/100\n",
            "21/21 [==============================] - 5s 243ms/step - loss: 5.3448 - accuracy: 0.0337\n",
            "Epoch 16/100\n",
            "21/21 [==============================] - 7s 309ms/step - loss: 5.2502 - accuracy: 0.0322\n",
            "Epoch 17/100\n",
            "21/21 [==============================] - 5s 235ms/step - loss: 5.1341 - accuracy: 0.0413\n",
            "Epoch 18/100\n",
            "21/21 [==============================] - 7s 326ms/step - loss: 5.0215 - accuracy: 0.0429\n",
            "Epoch 19/100\n",
            "21/21 [==============================] - 5s 240ms/step - loss: 4.9194 - accuracy: 0.0505\n",
            "Epoch 20/100\n",
            "21/21 [==============================] - 6s 293ms/step - loss: 4.8006 - accuracy: 0.0597\n",
            "Epoch 21/100\n",
            "21/21 [==============================] - 6s 267ms/step - loss: 4.6864 - accuracy: 0.0658\n",
            "Epoch 22/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 4.5787 - accuracy: 0.0643\n",
            "Epoch 23/100\n",
            "21/21 [==============================] - 7s 326ms/step - loss: 4.4768 - accuracy: 0.0750\n",
            "Epoch 24/100\n",
            "21/21 [==============================] - 5s 237ms/step - loss: 4.3661 - accuracy: 0.1087\n",
            "Epoch 25/100\n",
            "21/21 [==============================] - 7s 327ms/step - loss: 4.2751 - accuracy: 0.0949\n",
            "Epoch 26/100\n",
            "21/21 [==============================] - 5s 236ms/step - loss: 4.1651 - accuracy: 0.1240\n",
            "Epoch 27/100\n",
            "21/21 [==============================] - 5s 251ms/step - loss: 4.0695 - accuracy: 0.1317\n",
            "Epoch 28/100\n",
            "21/21 [==============================] - 6s 301ms/step - loss: 3.9754 - accuracy: 0.1501\n",
            "Epoch 29/100\n",
            "21/21 [==============================] - 6s 278ms/step - loss: 3.8852 - accuracy: 0.1639\n",
            "Epoch 30/100\n",
            "21/21 [==============================] - 7s 328ms/step - loss: 3.8068 - accuracy: 0.1792\n",
            "Epoch 31/100\n",
            "21/21 [==============================] - 5s 239ms/step - loss: 3.7297 - accuracy: 0.2006\n",
            "Epoch 32/100\n",
            "21/21 [==============================] - 7s 331ms/step - loss: 3.6383 - accuracy: 0.2221\n",
            "Epoch 33/100\n",
            "21/21 [==============================] - 5s 240ms/step - loss: 3.5693 - accuracy: 0.2343\n",
            "Epoch 34/100\n",
            "21/21 [==============================] - 5s 257ms/step - loss: 3.4867 - accuracy: 0.2833\n",
            "Epoch 35/100\n",
            "21/21 [==============================] - 6s 301ms/step - loss: 3.4017 - accuracy: 0.2971\n",
            "Epoch 36/100\n",
            "21/21 [==============================] - 5s 236ms/step - loss: 3.3301 - accuracy: 0.3047\n",
            "Epoch 37/100\n",
            "21/21 [==============================] - 7s 329ms/step - loss: 3.2605 - accuracy: 0.3292\n",
            "Epoch 38/100\n",
            "21/21 [==============================] - 7s 326ms/step - loss: 3.1815 - accuracy: 0.3568\n",
            "Epoch 39/100\n",
            "21/21 [==============================] - 7s 340ms/step - loss: 3.1132 - accuracy: 0.3844\n",
            "Epoch 40/100\n",
            "21/21 [==============================] - 5s 245ms/step - loss: 3.0527 - accuracy: 0.4181\n",
            "Epoch 41/100\n",
            "21/21 [==============================] - 6s 311ms/step - loss: 2.9794 - accuracy: 0.4548\n",
            "Epoch 42/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 2.9153 - accuracy: 0.4732\n",
            "Epoch 43/100\n",
            "21/21 [==============================] - 5s 247ms/step - loss: 2.8432 - accuracy: 0.4839\n",
            "Epoch 44/100\n",
            "21/21 [==============================] - 7s 310ms/step - loss: 2.7826 - accuracy: 0.5130\n",
            "Epoch 45/100\n",
            "21/21 [==============================] - 5s 243ms/step - loss: 2.7196 - accuracy: 0.5498\n",
            "Epoch 46/100\n",
            "21/21 [==============================] - 7s 329ms/step - loss: 2.6609 - accuracy: 0.5498\n",
            "Epoch 47/100\n",
            "21/21 [==============================] - 5s 242ms/step - loss: 2.5869 - accuracy: 0.5789\n",
            "Epoch 48/100\n",
            "21/21 [==============================] - 6s 307ms/step - loss: 2.5294 - accuracy: 0.5942\n",
            "Epoch 49/100\n",
            "21/21 [==============================] - 5s 250ms/step - loss: 2.4724 - accuracy: 0.6202\n",
            "Epoch 50/100\n",
            "21/21 [==============================] - 5s 237ms/step - loss: 2.4092 - accuracy: 0.6340\n",
            "Epoch 51/100\n",
            "21/21 [==============================] - 7s 327ms/step - loss: 2.3591 - accuracy: 0.6493\n",
            "Epoch 52/100\n",
            "21/21 [==============================] - 5s 242ms/step - loss: 2.2968 - accuracy: 0.6616\n",
            "Epoch 53/100\n",
            "21/21 [==============================] - 7s 328ms/step - loss: 2.2400 - accuracy: 0.6799\n",
            "Epoch 54/100\n",
            "21/21 [==============================] - 5s 240ms/step - loss: 2.1809 - accuracy: 0.7060\n",
            "Epoch 55/100\n",
            "21/21 [==============================] - 6s 273ms/step - loss: 2.1242 - accuracy: 0.7182\n",
            "Epoch 56/100\n",
            "21/21 [==============================] - 6s 281ms/step - loss: 2.0893 - accuracy: 0.7228\n",
            "Epoch 57/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 2.0310 - accuracy: 0.7182\n",
            "Epoch 58/100\n",
            "21/21 [==============================] - 7s 321ms/step - loss: 1.9797 - accuracy: 0.7305\n",
            "Epoch 59/100\n",
            "21/21 [==============================] - 5s 239ms/step - loss: 1.9345 - accuracy: 0.7534\n",
            "Epoch 60/100\n",
            "21/21 [==============================] - 7s 322ms/step - loss: 1.8739 - accuracy: 0.7810\n",
            "Epoch 61/100\n",
            "21/21 [==============================] - 5s 239ms/step - loss: 1.8404 - accuracy: 0.7749\n",
            "Epoch 62/100\n",
            "21/21 [==============================] - 5s 258ms/step - loss: 1.7850 - accuracy: 0.7856\n",
            "Epoch 63/100\n",
            "21/21 [==============================] - 6s 295ms/step - loss: 1.7465 - accuracy: 0.8009\n",
            "Epoch 64/100\n",
            "21/21 [==============================] - 5s 236ms/step - loss: 1.7093 - accuracy: 0.8040\n",
            "Epoch 65/100\n",
            "21/21 [==============================] - 7s 323ms/step - loss: 1.6775 - accuracy: 0.8086\n",
            "Epoch 66/100\n",
            "21/21 [==============================] - 5s 235ms/step - loss: 1.6226 - accuracy: 0.8070\n",
            "Epoch 67/100\n",
            "21/21 [==============================] - 6s 303ms/step - loss: 1.5727 - accuracy: 0.8270\n",
            "Epoch 68/100\n",
            "21/21 [==============================] - 5s 246ms/step - loss: 1.5263 - accuracy: 0.8346\n",
            "Epoch 69/100\n",
            "21/21 [==============================] - 5s 237ms/step - loss: 1.4949 - accuracy: 0.8377\n",
            "Epoch 70/100\n",
            "21/21 [==============================] - 7s 318ms/step - loss: 1.4629 - accuracy: 0.8423\n",
            "Epoch 71/100\n",
            "21/21 [==============================] - 5s 241ms/step - loss: 1.4191 - accuracy: 0.8545\n",
            "Epoch 72/100\n",
            "21/21 [==============================] - 7s 328ms/step - loss: 1.3823 - accuracy: 0.8591\n",
            "Epoch 73/100\n",
            "21/21 [==============================] - 5s 242ms/step - loss: 1.3470 - accuracy: 0.8591\n",
            "Epoch 74/100\n",
            "21/21 [==============================] - 6s 285ms/step - loss: 1.3089 - accuracy: 0.8821\n",
            "Epoch 75/100\n",
            "21/21 [==============================] - 6s 269ms/step - loss: 1.2749 - accuracy: 0.8790\n",
            "Epoch 76/100\n",
            "21/21 [==============================] - 5s 242ms/step - loss: 1.2438 - accuracy: 0.8821\n",
            "Epoch 77/100\n",
            "21/21 [==============================] - 7s 327ms/step - loss: 1.2154 - accuracy: 0.8897\n",
            "Epoch 78/100\n",
            "21/21 [==============================] - 5s 242ms/step - loss: 1.1960 - accuracy: 0.8867\n",
            "Epoch 79/100\n",
            "21/21 [==============================] - 7s 328ms/step - loss: 1.1508 - accuracy: 0.9005\n",
            "Epoch 80/100\n",
            "21/21 [==============================] - 7s 332ms/step - loss: 1.1220 - accuracy: 0.9005\n",
            "Epoch 81/100\n",
            "21/21 [==============================] - 7s 318ms/step - loss: 1.1051 - accuracy: 0.9020\n",
            "Epoch 82/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 1.0658 - accuracy: 0.9035\n",
            "Epoch 83/100\n",
            "21/21 [==============================] - 6s 273ms/step - loss: 1.0440 - accuracy: 0.9142\n",
            "Epoch 84/100\n",
            "21/21 [==============================] - 6s 287ms/step - loss: 1.0116 - accuracy: 0.9204\n",
            "Epoch 85/100\n",
            "21/21 [==============================] - 5s 239ms/step - loss: 0.9968 - accuracy: 0.9311\n",
            "Epoch 86/100\n",
            "21/21 [==============================] - 7s 328ms/step - loss: 0.9634 - accuracy: 0.9326\n",
            "Epoch 87/100\n",
            "21/21 [==============================] - 5s 239ms/step - loss: 0.9433 - accuracy: 0.9280\n",
            "Epoch 88/100\n",
            "21/21 [==============================] - 7s 317ms/step - loss: 0.9311 - accuracy: 0.9326\n",
            "Epoch 89/100\n",
            "21/21 [==============================] - 5s 233ms/step - loss: 0.9055 - accuracy: 0.9387\n",
            "Epoch 90/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 0.8835 - accuracy: 0.9372\n",
            "Epoch 91/100\n",
            "21/21 [==============================] - 7s 316ms/step - loss: 0.8539 - accuracy: 0.9342\n",
            "Epoch 92/100\n",
            "21/21 [==============================] - 5s 238ms/step - loss: 0.8328 - accuracy: 0.9387\n",
            "Epoch 93/100\n",
            "21/21 [==============================] - 7s 331ms/step - loss: 0.8104 - accuracy: 0.9510\n",
            "Epoch 94/100\n",
            "21/21 [==============================] - 5s 241ms/step - loss: 0.7949 - accuracy: 0.9372\n",
            "Epoch 95/100\n",
            "21/21 [==============================] - 6s 304ms/step - loss: 0.7788 - accuracy: 0.9495\n",
            "Epoch 96/100\n",
            "21/21 [==============================] - 6s 250ms/step - loss: 0.7603 - accuracy: 0.9433\n",
            "Epoch 97/100\n",
            "21/21 [==============================] - 5s 249ms/step - loss: 0.7314 - accuracy: 0.9541\n",
            "Epoch 98/100\n",
            "21/21 [==============================] - 7s 321ms/step - loss: 0.7216 - accuracy: 0.9495\n",
            "Epoch 99/100\n",
            "21/21 [==============================] - 5s 245ms/step - loss: 0.7076 - accuracy: 0.9510\n",
            "Epoch 100/100\n",
            "21/21 [==============================] - 7s 326ms/step - loss: 0.6880 - accuracy: 0.9541\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7df0d1bc89a0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Deep learning models\"\n",
        "for i in range(10):\n",
        "\n",
        "    next_word = predict_next_word(model_word2vec_LSTM, tokenizer, seed_text, max_sequence_len)\n",
        "    seed_text += \" \" + next_word\n",
        "    print(\"text: \", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFr8bVEuCKm",
        "outputId": "1c922e16-0323-47aa-d556-28af322c9725"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  Deep learning models approach\n",
            "text:  Deep learning models approach based\n",
            "text:  Deep learning models approach based transformation\n",
            "text:  Deep learning models approach based transformation made\n",
            "text:  Deep learning models approach based transformation made obsolete\n",
            "text:  Deep learning models approach based transformation made obsolete room\n",
            "text:  Deep learning models approach based transformation made obsolete room intermediate\n",
            "text:  Deep learning models approach based transformation made obsolete room intermediate given\n",
            "text:  Deep learning models approach based transformation made obsolete room intermediate given collection\n",
            "text:  Deep learning models approach based transformation made obsolete room intermediate given collection rule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN or not, not a lot of difference, lets\n",
        "#Evaluate"
      ],
      "metadata": {
        "id": "XgWFd7mcxPwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_sequence_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha-E_HF6QIaM",
        "outputId": "a2c46ace-b9e4-4945-943e-be0cc45185ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "test_data = \"machine learning is a field of study in artificial intelligence concerned with the development and\"\n",
        "\n",
        "# Tokenize the text\n",
        "token_list = tokenizer.texts_to_sequences([test_data])[0]\n",
        "\n",
        "# Ensure token list has enough tokens to form a sequence and a label\n",
        "if len(token_list) > 1:\n",
        "    # Prepare input sequences and labels\n",
        "    input_sequences = []\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # Pad sequences to ensure uniform length\n",
        "    max_sequence_len = max(len(x) for x in input_sequences)  # Find max length to pad\n",
        "    padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "    # Predictors and actual next words\n",
        "    predictors = padded_sequences[:, :-1]\n",
        "    labels = padded_sequences[:, -1]\n",
        "\n",
        "    # Predict the probability distribution for each input sequence\n",
        "    predicted_probs_LSTM = model_word2vec_LSTM.predict(predictors, verbose=0)\n",
        "    predicted_probs_RNN = model_word2vec_RNN.predict(predictors, verbose=0)\n",
        "\n",
        "    # Convert labels to categorical for all words in each sequence\n",
        "    actual_labels = to_categorical(labels, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "    # Calculate cross-entropy loss for each prediction against its actual next word\n",
        "    loss_LSTM = tf.keras.losses.categorical_crossentropy(actual_labels, predicted_probs_LSTM)\n",
        "    loss_RNN = tf.keras.losses.categorical_crossentropy(actual_labels, predicted_probs_RNN)\n",
        "    perplexity_LSTM = tf.exp(tf.reduce_mean(loss_LSTM))\n",
        "    perplexity_RNN = tf.exp(tf.reduce_mean(loss_RNN))\n",
        "    print(\"Perplexity_LSTM:\", perplexity_LSTM.numpy())\n",
        "    print(\"Perplexity_RNN:\", perplexity_RNN.numpy())\n",
        "else:\n",
        "    print(\"Not enough data to predict the next word.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y26PvreNv1N2",
        "outputId": "9eba3345-2aa7-46cd-dbfa-c0b7864a66bd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity_LSTM: 1753.0775\n",
            "Perplexity_RNN: 4364.5254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On test data that is not part of the corpus, the LSTM makes better predictions."
      ],
      "metadata": {
        "id": "P2MYJ89AVotr"
      }
    }
  ]
}